{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Intensive Applications & Apache Spark Core\n",
    "\n",
    "## Run in GitHub Codespaces\n",
    "\n",
    "**Course**: DSCC 202-402 - Data Science at Scale  \n",
    "**Topics**: Big Data Fundamentals, Team Roles, Spark Architecture, DataFrames, Operations, & Advanced Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Sample Data for Demonstrations\n",
    "\n",
    "**Note**: Databricks notebooks have `spark` pre-initialized, so we don't need to create a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, count, when, max, min, expr, lit, datediff, current_date\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Core Lecture - Review\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get Spark Context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "ui_url = spark.sparkContext.uiWebUrl\n",
    "print(f\"Spark UI available at: {ui_url}\")\n",
    "print(\"üí° In GitHub Codespaces: Check the 'PORTS' tab below for forwarded port 4040 to access Spark UI\")\n",
    "\n",
    "# In Databricks, spark is already available\n",
    "# For local testing: spark = SparkSession.builder.appName(\"SparkCoreLecture\").getOrCreate()\n",
    "\n",
    "# Sample customer data\n",
    "customers_data = [\n",
    "    (\"C001\", \"Alice\", 25, \"CA\"), (\"C002\", \"Bob\", 30, \"NY\"),\n",
    "    (\"C003\", \"Carol\", 28, \"CA\"), (\"C004\", \"Dave\", 35, \"TX\"),\n",
    "    (\"C005\", \"Eve\", 22, \"NY\"), (\"C006\", \"Frank\", 45, \"CA\")\n",
    "]\n",
    "customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"name\", \"age\", \"state\"])\n",
    "\n",
    "# Sample transaction data\n",
    "transactions_data = [\n",
    "    (\"T001\", \"C001\", 100.0, \"Electronics\"), (\"T002\", \"C001\", 50.0, \"Books\"),\n",
    "    (\"T003\", \"C002\", 200.0, \"Electronics\"), (\"T004\", \"C003\", 150.0, \"Books\"),\n",
    "    (\"T005\", \"C002\", 75.0, \"Home\"), (\"T006\", \"C004\", 300.0, \"Electronics\"),\n",
    "    (\"T007\", \"C005\", 25.0, \"Books\"), (\"T008\", \"C001\", 120.0, \"Home\")\n",
    "]\n",
    "transactions_df = spark.createDataFrame(transactions_data, [\"txn_id\", \"customer_id\", \"amount\", \"category\"])\n",
    "\n",
    "print(\"‚úÖ Sample data created for demonstrations\")\n",
    "print(f\"   Customers: {customers_df.count()} rows\")\n",
    "print(f\"   Transactions: {transactions_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Course Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Data-Intensive Applications?\n",
    "\n",
    "### Key Characteristics:\n",
    "- Processes data that doesn't fit on a single machine\n",
    "- Uses distributed computing frameworks\n",
    "- Automates or augments decision-making\n",
    "- Requires specialized infrastructure and tools\n",
    "\n",
    "### Examples:\n",
    "- Recommendation systems (Netflix, Amazon)\n",
    "- Fraud detection (banks, credit cards)\n",
    "- Search engines (Google, Bing)\n",
    "- Real-time analytics dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals: Augment vs Replace Human Decision-Making\n",
    "\n",
    "### The Reality:\n",
    "- Data-intensive applications typically **augment** human capabilities\n",
    "- Humans provide context, judgment, and domain expertise\n",
    "- Systems provide scale, consistency, and data-driven insights\n",
    "- Best outcomes come from human-machine collaboration\n",
    "\n",
    "### Data Scale Considerations:\n",
    "- Data-intensive applications handle data that exceeds single-machine capacity\n",
    "- Require distributed processing across multiple machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Roadmap\n",
    "\n",
    "### Part 1: Foundations (Sections 1-4)\n",
    "1. Big Data fundamentals (5 V's, CRISP-DM)\n",
    "2. Team roles and collaboration\n",
    "3. Development best practices\n",
    "\n",
    "### Part 2: Spark Architecture (Sections 5-6)\n",
    "4. Cluster architecture (Driver, Executors)\n",
    "5. Core concepts (RDDs, lazy evaluation, immutability)\n",
    "\n",
    "### Part 3: Practical Skills (Sections 7-8)\n",
    "6. DataFrame operations\n",
    "7. Advanced topics (SQL, UDFs, caching)\n",
    "\n",
    "### Part 4: Integration (Section 9)\n",
    "8. Real-world examples and best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Big Data Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 4 V's of Big Data\n",
    "\n",
    "![4 V's of Big Data](../illustrations/4vs_of_big_data.png)\n",
    "\n",
    "### 1. **Volume** - The size of the data being processed\n",
    "- Terabytes, petabytes, exabytes of data\n",
    "- Example: Walmart processes 2.5 petabytes of data hourly\n",
    "\n",
    "### 2. **Velocity** - The speed at which data is generated and processed\n",
    "- Real-time or near real-time processing requirements\n",
    "- Example: Twitter handles 500 million tweets per day\n",
    "\n",
    "### 3. **Variety** - The different forms and sources of data\n",
    "- Structured (databases), semi-structured (JSON), unstructured (text, images)\n",
    "- Example: Social media data includes text, images, videos, metadata\n",
    "\n",
    "### 4. **Veracity** - The uncertainty or quality of the data\n",
    "- Data accuracy, trustworthiness, reliability\n",
    "- Example: Sensor data may have noise or missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP-DM: Cross-Industry Standard Process for Data Mining\n",
    "\n",
    "### The 6 Phases:\n",
    "![CRISP-DM](../illustrations/crispdm.png)\n",
    "\n",
    "1. **Business Understanding**: Define objectives and requirements\n",
    "2. **Data Understanding**: Collect and explore initial data\n",
    "3. **Data Preparation**: Clean, transform, and format data\n",
    "4. **Modeling**: Build and train models\n",
    "5. **Evaluation**: Assess model performance against business goals\n",
    "6. **Deployment**: Put model into production\n",
    "\n",
    "**Key Insight**: This is an iterative cycle, not a linear process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Considerations\n",
    "\n",
    "### When Do You Need Distributed Computing?\n",
    "\n",
    "### Indicators You Need Distributed Processing:\n",
    "- Data doesn't fit in memory on a single machine (> 16-32GB typical laptop RAM)\n",
    "- Processing time exceeds acceptable limits on single machine\n",
    "- Need for fault tolerance and high availability\n",
    "- Multiple data sources requiring parallel ingestion\n",
    "\n",
    "### Enter Apache Spark:\n",
    "- Processes terabytes to petabytes of data\n",
    "- Distributes computation across hundreds/thousands of machines\n",
    "- Provides fault tolerance through lineage tracking\n",
    "- Offers unified APIs for batch and streaming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Frequency/Impact Matrix\n",
    "\n",
    "### The Four Quadrants:\n",
    "\n",
    "| Impact \\ Frequency | **High Frequency** | **Low Frequency** |\n",
    "|-------------------|-------------------|------------------|\n",
    "| **High Impact** | üåü **Highest Value** <br> Many high-stakes decisions <br> Example: Automated trading | ‚ö†Ô∏è **Strategic Decisions** <br> Infrequent but critical <br> Example: Merger analysis |\n",
    "| **Low Impact** | ‚úÖ **Valuable Target** <br> Small individual impact, <br> large cumulative value <br> Example: Product recommendations | ‚ùå **Not Worth Building** <br> Low frequency, low impact <br> Example: One-off reports |\n",
    "\n",
    "### Key Insights:\n",
    "- **High Frequency/High Impact**: Maximum ROI for data applications\n",
    "- **High Frequency/Low Impact**: Still valuable due to cumulative effect\n",
    "- **Low Frequency/High Impact**: Less common, may need human oversight\n",
    "- **Low Frequency/Low Impact**: Generally not worth automation cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Data Team Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Five Key Roles in Data-Intensive Projects\n",
    "\n",
    "![Roles](../illustrations/roles.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role 1: Data Analyst\n",
    "\n",
    "### Key Activities:\n",
    "- Explore datasets to understand patterns and trends\n",
    "- Create visualizations and dashboards\n",
    "- Generate insights from data\n",
    "- Answer business questions with data\n",
    "- Communicate findings to stakeholders\n",
    "\n",
    "### Tools:\n",
    "- SQL, Python/R\n",
    "- Tableau, Power BI\n",
    "- Jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role 2: Data Engineer\n",
    "\n",
    "### Key Activities:\n",
    "- Design and implement data pipelines (ETL/ELT)\n",
    "- Manage data infrastructure\n",
    "- Ensure data quality and reliability\n",
    "- Optimize data storage and access\n",
    "- Handle data at scale\n",
    "\n",
    "### Tools:\n",
    "- Apache Spark, Airflow\n",
    "- Delta Lake, Databricks\n",
    "- Cloud platforms (AWS, Azure, GCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role 3: Machine Learning Engineer\n",
    "\n",
    "### Key Activities:\n",
    "- Design and train ML models\n",
    "- Feature engineering\n",
    "- Model optimization and tuning\n",
    "- Deploy models to production\n",
    "- Monitor model performance\n",
    "\n",
    "### Tools:\n",
    "- Scikit-learn, TensorFlow, PyTorch\n",
    "- MLflow, Kubeflow\n",
    "- Model serving platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role 4: Evaluation Engineer\n",
    "\n",
    "### Key Activities:\n",
    "- Design evaluation metrics\n",
    "- Test model accuracy and reliability\n",
    "- Monitor models in production\n",
    "- Detect model drift and degradation\n",
    "- Ensure models meet business requirements\n",
    "\n",
    "### Tools:\n",
    "- A/B testing frameworks\n",
    "- Monitoring systems\n",
    "- Statistical analysis tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role 5: Domain Expert\n",
    "\n",
    "### Why Domain Experts Are Crucial:\n",
    "- Provide business context for data\n",
    "- Validate insights and models\n",
    "- Identify relevant features and patterns\n",
    "- Understand limitations and biases in data\n",
    "- Translate between technical and business stakeholders\n",
    "\n",
    "### Without Domain Experts:\n",
    "- ‚ùå Technical solutions may not address real business needs\n",
    "- ‚ùå Important patterns may be missed\n",
    "- ‚ùå Models may make unrealistic assumptions\n",
    "- ‚ùå Results may be misinterpreted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Collaboration Patterns\n",
    "\n",
    "### Effective Data Teams:\n",
    "- **Cross-functional**: All roles work together\n",
    "- **Iterative**: Follow CRISP-DM cycle\n",
    "- **Communicative**: Regular sync between roles\n",
    "- **Adaptable**: Respond to new insights and challenges\n",
    "\n",
    "### Typical Workflow:\n",
    "1. **Domain Expert** + **Data Analyst**: Define problem and explore data\n",
    "2. **Data Engineer**: Build pipelines to prepare data\n",
    "3. **ML Engineer**: Build and train models\n",
    "4. **Evaluation Engineer**: Test model performance\n",
    "5. **Data Engineer**: Deploy to production\n",
    "6. **All roles**: Monitor, iterate, and improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Development Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Pitfalls in Data-Intensive Applications\n",
    "\n",
    "![10 Ways Data Projects Fail](../illustrations/10_ways_data_projects_fail.jpeg)\n",
    "\n",
    "### Top Pitfalls to Avoid:\n",
    "1. **Lack of clear objectives** - Not defining success criteria\n",
    "2. **Insufficient data quality** - Garbage in, garbage out\n",
    "3. **Ignoring ethical considerations** - Bias, privacy, fairness\n",
    "4. **Poor team collaboration** - Silos between roles\n",
    "5. **Inadequate infrastructure** - Can't handle scale\n",
    "6. **No deployment plan** - Models never reach production\n",
    "7. **Lack of monitoring** - Models degrade over time\n",
    "8. **Overfitting** - Model doesn't generalize\n",
    "9. **Underestimating complexity** - Technical debt accumulates\n",
    "10. **Ignoring business context** - Solutions don't address real needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Best Practices\n",
    "\n",
    "### What Makes a Good Development Process:\n",
    "- ‚úÖ **Well-defined**: Clear stages and deliverables\n",
    "- ‚úÖ **Flexible**: Adapts to new insights and changes\n",
    "- ‚úÖ **Iterative**: Follows CRISP-DM cycle\n",
    "- ‚úÖ **Collaborative**: Involves all stakeholders\n",
    "\n",
    "### Other Best Practices:\n",
    "- Clear objectives and success metrics\n",
    "- Data quality checks and validation\n",
    "- Ethical considerations (bias, privacy, transparency)\n",
    "- Version control for code and data\n",
    "- Automated testing and monitoring\n",
    "- Documentation and knowledge sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Models Fail: Next Steps\n",
    "\n",
    "### Why More Data Is Often the Answer:\n",
    "- More training examples improve model generalization\n",
    "- Additional features may capture important patterns\n",
    "- More diverse data helps handle edge cases\n",
    "- Larger datasets reduce overfitting\n",
    "\n",
    "### The CRISP-DM Response to Failure:\n",
    "1. **Evaluate**: Understand why the model failed\n",
    "2. **Data Understanding**: Identify data gaps or quality issues\n",
    "3. **Data Preparation**: Acquire more data or improve existing data\n",
    "4. **Modeling**: Retrain with enhanced dataset\n",
    "5. **Evaluation**: Test again\n",
    "\n",
    "### Not Good First Steps:\n",
    "- ‚ùå Immediately change business goals (addresses symptoms, not cause)\n",
    "- ‚ùå Accept poor performance (defeats the purpose)\n",
    "- ‚ùå Abandon the project (too drastic without investigation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical Considerations and Data Quality\n",
    "\n",
    "### Ethical Principles:\n",
    "- **Fairness**: Avoid bias and discrimination\n",
    "- **Transparency**: Explain how decisions are made\n",
    "- **Privacy**: Protect sensitive information\n",
    "- **Accountability**: Take responsibility for outcomes\n",
    "\n",
    "### Data Quality Dimensions:\n",
    "- **Accuracy**: Data correctly represents reality\n",
    "- **Completeness**: No critical missing values\n",
    "- **Consistency**: Data is consistent across sources\n",
    "- **Timeliness**: Data is up-to-date\n",
    "- **Validity**: Data conforms to expected formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Success Factors Summary\n",
    "\n",
    "### To Build Successful Data-Intensive Applications:\n",
    "\n",
    "1. **Clear Vision**: Well-defined objectives aligned with business goals\n",
    "2. **Strong Team**: All 5 roles working collaboratively\n",
    "3. **Quality Data**: Accurate, complete, and relevant data\n",
    "4. **Robust Process**: Iterative development following CRISP-DM\n",
    "5. **Appropriate Technology**: Tools that scale (like Apache Spark!)\n",
    "6. **Ethical Framework**: Consider fairness, privacy, and accountability\n",
    "7. **Continuous Monitoring**: Track performance and iterate\n",
    "8. **Production Focus**: Plan for deployment from day one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Apache Spark Architecture\n",
    "\n",
    "Now we transition from general principles to Apache Spark specifics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Apache Spark?\n",
    "\n",
    "### Definition:\n",
    "Apache Spark is a **unified analytics engine for large-scale data processing**.\n",
    "\n",
    "### Key Features:\n",
    "- **Speed**: Up to 100x faster than Hadoop MapReduce\n",
    "- **Ease of Use**: High-level APIs in Python, Scala, Java, R\n",
    "- **Unified**: Supports batch processing, streaming, ML, graph processing\n",
    "- **Fault Tolerant**: Automatically recovers from failures\n",
    "- **Scalable**: Runs on single machine to thousands of nodes\n",
    "\n",
    "### When to Use Spark:\n",
    "- Processing terabytes to petabytes of data\n",
    "- Need for distributed computing\n",
    "- Complex data transformations and analytics\n",
    "- Machine learning at scale\n",
    "- Real-time streaming applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark API Hierarchy\n",
    "\n",
    "![Spark API](../illustrations/spark_api.png)\n",
    "\n",
    "### The API Stack (Bottom to Top):\n",
    "\n",
    "1. **RDD** (Resilient Distributed Dataset) - Foundational abstraction\n",
    "   - Low-level API\n",
    "   - Full control over data and operations\n",
    "   - All higher APIs build on RDDs\n",
    "\n",
    "2. **DataFrame** - Structured data with named columns\n",
    "   - High-level API\n",
    "   - Automatic optimizations (Catalyst)\n",
    "   - Similar to database tables or pandas DataFrames\n",
    "\n",
    "3. **Dataset** - Type-safe DataFrames (Scala/Java only)\n",
    "   - Compile-time type checking\n",
    "   - Object-oriented programming interface\n",
    "\n",
    "4. **Specialized APIs**:\n",
    "   - Spark SQL: SQL queries on structured data\n",
    "   - MLlib: Machine learning library\n",
    "   - GraphX: Graph processing\n",
    "   - Structured Streaming: Real-time stream processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Cluster Architecture\n",
    "\n",
    "![Spark Cluster](../illustrations/spark_cluster.png)\n",
    "\n",
    "### Components:\n",
    "\n",
    "1. **Driver Program**: Your application code\n",
    "2. **Cluster Manager**: Allocates resources (YARN, Mesos, Kubernetes, Standalone)\n",
    "3. **Executors**: Processes that run computations and store data\n",
    "\n",
    "### How They Work Together:\n",
    "1. Driver sends application to cluster manager\n",
    "2. Cluster manager allocates executors on worker nodes\n",
    "3. Driver sends tasks to executors\n",
    "4. Executors run tasks and return results to driver\n",
    "5. Driver coordinates the entire application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Driver Node\n",
    "\n",
    "### Driver Responsibilities:\n",
    "- Runs the `main()` function of your application\n",
    "- Creates the SparkContext/SparkSession\n",
    "- Converts user program into tasks\n",
    "- Schedules tasks on executors\n",
    "- Tracks executor status\n",
    "- Returns results to the user\n",
    "\n",
    "### In Databricks:\n",
    "- The notebook runs on the driver\n",
    "- SparkSession (`spark`) is pre-initialized\n",
    "- You interact with the driver when running cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Driver operations\n",
    "# In Databricks, 'spark' is already created (this is the driver's SparkSession)\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# The driver coordinates all operations\n",
    "print(\"\\nDriver is ready to execute your Spark code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executor Nodes\n",
    "\n",
    "### Executor Responsibilities:\n",
    "- Execute tasks sent by the driver\n",
    "- Store data for caching/persistence\n",
    "- Report computation results back to driver\n",
    "- Report metrics (memory usage, task duration)\n",
    "\n",
    "### Key Characteristics:\n",
    "- Multiple executors per application\n",
    "- Each executor runs in its own JVM process\n",
    "- Executors are long-lived (for the application duration)\n",
    "- Failed executors are automatically restarted\n",
    "\n",
    "### Executor vs Driver:\n",
    "- **Driver**: ONE per application, orchestrates everything\n",
    "- **Executors**: MANY per application, do the actual work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Hierarchy: Job, Stage, Task, Partition\n",
    "\n",
    "![Spark Execution](../illustrations/spark_execution.png)\n",
    "\n",
    "### The Four Levels:\n",
    "\n",
    "1. **Job** - A sequence of stages triggered by an action\n",
    "   - One job per action (count, collect, save)\n",
    "   - Example: `df.count()` triggers one job\n",
    "\n",
    "2. **Stage** - A set of tasks in a DAG\n",
    "   - Stages are divided at shuffle boundaries\n",
    "   - All tasks in a stage can run in parallel\n",
    "   - Example: Stage 1 (read + filter), Stage 2 (groupBy + aggregate)\n",
    "\n",
    "3. **Task** - A unit of work sent to an executor\n",
    "   - One task per partition\n",
    "   - Tasks are the smallest unit of execution\n",
    "   - Example: Process partition 1 of 200\n",
    "\n",
    "4. **Partition** - An atomic chunk of data (logical division) stored on a node\n",
    "   - Data is split into partitions for parallel processing\n",
    "   - Each partition is processed independently\n",
    "   - More partitions = more parallelism\n",
    "\n",
    "### Relationship:\n",
    "```\n",
    "Action ‚Üí Job ‚Üí Stages ‚Üí Tasks ‚Üí Partitions\n",
    "  1        1       N        M       M\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Understanding partitions\n",
    "\n",
    "print(\"Default partitions:\")\n",
    "print(f\"  Customers: {customers_df.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"  Transactions: {transactions_df.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Create a larger dataset to demonstrate partitioning\n",
    "large_df = spark.range(0, 1000000)\n",
    "print(f\"\\nLarge dataset: {large_df.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"Total rows: {large_df.count():,}\")\n",
    "print(f\"Rows per partition: ~{large_df.count() // large_df.rdd.getNumPartitions():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Deep Dive\n",
    "\n",
    "### RDD = Resilient Distributed Dataset\n",
    "\n",
    "**Resilient**: Fault-tolerant, automatically recovers from failures\n",
    "- Uses lineage information to recompute lost partitions\n",
    "\n",
    "**Distributed**: Data is split across multiple nodes\n",
    "- Enables parallel processing\n",
    "\n",
    "**Dataset**: Collection of objects\n",
    "- Can contain any type of data (rows, tuples, custom objects)\n",
    "\n",
    "### RDD Properties:\n",
    "- **Immutable**: Cannot be changed after creation\n",
    "- **Lazy**: Transformations are not computed until an action is called\n",
    "- **Partitioned**: Divided for parallel processing\n",
    "- **Typed**: Contains elements of a specific type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating and using RDDs\n",
    "\n",
    "# Create RDD from a Python list\n",
    "numbers_rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "print(\"RDD created from list:\")\n",
    "print(f\"  Type: {type(numbers_rdd)}\")\n",
    "print(f\"  Partitions: {numbers_rdd.getNumPartitions()}\")\n",
    "print(f\"  Elements: {numbers_rdd.collect()}\")\n",
    "\n",
    "# RDD transformations (lazy)\n",
    "squared_rdd = numbers_rdd.map(lambda x: x ** 2)\n",
    "even_rdd = squared_rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "print(\"\\nAfter transformations (map and filter):\")\n",
    "print(f\"  Even squares: {even_rdd.collect()}\")\n",
    "\n",
    "# Convert DataFrame to RDD\n",
    "customers_rdd = customers_df.rdd\n",
    "print(f\"\\nDataFrame to RDD:\")\n",
    "print(f\"  Type: {type(customers_rdd)}\")\n",
    "print(f\"  First row: {customers_rdd.first()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 6: Core Spark Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immutability in Spark\n",
    "\n",
    "### What Does Immutability Mean?\n",
    "- Once created, a DataFrame **cannot be changed**\n",
    "- Transformations create **new** DataFrames\n",
    "- Original DataFrame remains unchanged\n",
    "\n",
    "### Why Immutability?\n",
    "1. **Fault Tolerance**: Can recreate data from lineage if node fails\n",
    "2. **Consistency**: Multiple operations see the same data\n",
    "3. **Optimization**: Engine can safely reorder operations\n",
    "4. **Parallelism**: Safe concurrent access without locks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Demonstrating immutability\n",
    "\n",
    "print(\"Original customers DataFrame:\")\n",
    "customers_df.show()\n",
    "\n",
    "# Apply transformation - creates NEW DataFrame\n",
    "customers_with_category = customers_df.withColumn(\n",
    "    \"age_category\",\n",
    "    when(col(\"age\") < 25, \"Young\")\n",
    "    .when(col(\"age\") < 35, \"Adult\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "\n",
    "print(\"\\nNew DataFrame with age_category:\")\n",
    "customers_with_category.show()\n",
    "\n",
    "print(\"\\nOriginal DataFrame is UNCHANGED (immutability):\")\n",
    "customers_df.show()\n",
    "\n",
    "print(f\"\\nOriginal DataFrame columns: {customers_df.columns}\")\n",
    "print(f\"New DataFrame columns: {customers_with_category.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Lineage and Fault Tolerance\n",
    "\n",
    "![RDD Lineage](../illustrations/rdd_lineage.png)\n",
    "\n",
    "### How Lineage Works:\n",
    "1. Spark tracks the **sequence of transformations** applied to data\n",
    "2. Creates a **Directed Acyclic Graph (DAG)** of dependencies\n",
    "3. If a partition is lost (node failure), Spark:\n",
    "   - Traces back through the lineage\n",
    "   - Recomputes only the lost partition\n",
    "   - Continues execution\n",
    "\n",
    "### Example Lineage:\n",
    "```\n",
    "Original Data ‚Üí filter() ‚Üí map() ‚Üí groupBy() ‚Üí Action\n",
    "```\n",
    "\n",
    "If partition fails during `groupBy()`, Spark recomputes:  \n",
    "`filter()` ‚Üí `map()` ‚Üí `groupBy()` for that partition only\n",
    "\n",
    "### Benefits:\n",
    "- No need for expensive data replication\n",
    "- Automatic recovery without user intervention\n",
    "- Efficient - only recomputes what's needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations vs Actions Overview\n",
    "\n",
    "### Placeholder Diagram: Transformations vs Actions Comparison Table\n",
    "\n",
    "| **Transformations (Lazy)** | **Actions (Eager)** |\n",
    "|---------------------------|---------------------|\n",
    "| Return new RDD/DataFrame | Return value to driver or write to storage |\n",
    "| Not computed immediately | Trigger immediate execution |\n",
    "| Build execution plan (DAG) | Execute the DAG |\n",
    "| Examples: | Examples: |\n",
    "| - `map()` | - `count()` |\n",
    "| - `filter()` | - `collect()` |\n",
    "| - `groupBy()` | - `show()` |\n",
    "| - `join()` | - `take()` |\n",
    "| - `select()` | - `first()` |\n",
    "| - `where()` | - `save()` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations: Lazy Operations\n",
    "\n",
    "### Common DataFrame Transformations:\n",
    "\n",
    "1. **select()**: Choose specific columns\n",
    "2. **filter() / where()**: Filter rows by condition\n",
    "3. **withColumn()**: Add or modify columns\n",
    "4. **groupBy()**: Group data for aggregation\n",
    "5. **join()**: Combine DataFrames\n",
    "6. **union()**: Concatenate DataFrames\n",
    "7. **orderBy() / sort()**: Sort data\n",
    "8. **distinct()**: Remove duplicates\n",
    "\n",
    "### Key Characteristic:\n",
    "- **Nothing is computed** when you call a transformation\n",
    "- Spark just records the operation in the execution plan\n",
    "- Actual computation happens when an action is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Chaining transformations (all lazy)\n",
    "\n",
    "print(\"Defining transformation chain (no computation yet)...\\n\")\n",
    "\n",
    "# Chain of transformations\n",
    "result_df = transactions_df \\\n",
    "    .filter(col(\"amount\") > 50) \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(count(\"*\").alias(\"transaction_count\"),\n",
    "         sum(\"amount\").alias(\"total_amount\")) \\\n",
    "    .orderBy(col(\"total_amount\").desc())\n",
    "\n",
    "print(\"‚úÖ Transformations defined (not executed yet)\")\n",
    "print(f\"Result type: {type(result_df)}\")\n",
    "print(\"\\nNo actual data processing has occurred until we call an action!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions: Eager Operations\n",
    "\n",
    "### Common DataFrame Actions:\n",
    "\n",
    "1. **show()**: Display rows in console\n",
    "2. **count()**: Count number of rows\n",
    "3. **collect()**: Return all rows to driver\n",
    "4. **take(n)**: Return first n rows\n",
    "5. **first()**: Return first row\n",
    "6. **write()**: Save to storage\n",
    "7. **foreach()**: Apply function to each row\n",
    "\n",
    "### Key Characteristic:\n",
    "- **Triggers immediate execution** of all pending transformations\n",
    "- Returns results to the driver program\n",
    "- Can see output or write to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Actions trigger execution\n",
    "\n",
    "print(\"Now calling an ACTION - this triggers execution...\\n\")\n",
    "\n",
    "# This action triggers the entire transformation chain\n",
    "result_df.show()\n",
    "\n",
    "print(\"\\n‚úÖ Action completed - all transformations were executed!\")\n",
    "\n",
    "# Other actions\n",
    "print(f\"\\nNumber of categories: {result_df.count()}\")\n",
    "print(f\"First row: {result_df.first()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation Explained\n",
    "\n",
    "### How Lazy Evaluation Works:\n",
    "\n",
    "1. **User writes code** with transformations\n",
    "2. **Spark builds DAG** (Directed Acyclic Graph) of operations\n",
    "3. **No computation** happens yet\n",
    "4. **User calls action**\n",
    "5. **Spark optimizes** the DAG\n",
    "6. **Execution begins** and data is processed\n",
    "7. **Results returned** to user\n",
    "\n",
    "### Benefits:\n",
    "- **Optimization**: Spark can rearrange operations for efficiency\n",
    "- **Performance**: Avoids unnecessary intermediate results\n",
    "- **Resource Efficiency**: Only loads data that's needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Demonstrating lazy evaluation with timing\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LAZY EVALUATION DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a larger dataset for timing\n",
    "large_data = spark.range(0, 1000000)\n",
    "\n",
    "print(\"\\n1. Defining transformations (should be instant)...\")\n",
    "start = time.time()\n",
    "\n",
    "# Chain of transformations - all lazy\n",
    "transformed = large_data \\\n",
    "    .filter(col(\"id\") % 2 == 0) \\\n",
    "    .withColumn(\"squared\", col(\"id\") * col(\"id\")) \\\n",
    "    .filter(col(\"squared\") < 1000000)\n",
    "\n",
    "transform_time = time.time() - start\n",
    "print(f\"   Time to define transformations: {transform_time:.4f} seconds\")\n",
    "print(\"   ‚úì No computation occurred (lazy!)\")\n",
    "\n",
    "print(\"\\n2. Calling action (triggers execution)...\")\n",
    "start = time.time()\n",
    "\n",
    "# This action triggers all transformations\n",
    "result_count = transformed.count()\n",
    "\n",
    "action_time = time.time() - start\n",
    "print(f\"   Time to execute count(): {action_time:.4f} seconds\")\n",
    "print(f\"   Result: {result_count:,} rows\")\n",
    "print(\"   ‚úì All transformations executed!\")\n",
    "\n",
    "print(f\"\\nüìä Comparison:\")\n",
    "print(f\"   Transformation definition: {transform_time:.4f}s (instant - lazy)\")\n",
    "print(f\"   Action execution: {action_time:.4f}s (actual work - eager)\")\n",
    "print(f\"   Speed ratio: {action_time/transform_time:.0f}x slower for action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Lazy Evaluation\n",
    "\n",
    "### 1. Query Optimization\n",
    "Spark can analyze the entire chain and optimize:\n",
    "- **Predicate pushdown**: Move filters earlier\n",
    "- **Column pruning**: Only read needed columns\n",
    "- **Join reordering**: Optimize join sequence\n",
    "\n",
    "### 2. Avoiding Unnecessary Work\n",
    "```python\n",
    "df.filter(...).filter(...).take(10)  # Only processes enough data for 10 rows\n",
    "```\n",
    "\n",
    "### 3. Pipeline Optimization\n",
    "Multiple operations can be combined into single stage:\n",
    "```python\n",
    "df.select(...).filter(...).map(...)  # May execute in single pass\n",
    "```\n",
    "\n",
    "### 4. Fault Tolerance\n",
    "Lineage graph enables recomputation without storing intermediate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames vs RDDs\n",
    "\n",
    "### Why DataFrames Are Optimized:\n",
    "\n",
    "1. **Catalyst Optimizer**\n",
    "   - Analyzes and optimizes query plans\n",
    "   - Pushes filters and projections down\n",
    "   - Reorders operations for efficiency\n",
    "\n",
    "2. **Tungsten Execution Engine**\n",
    "   - Off-heap memory management\n",
    "   - Cache-aware computation\n",
    "   - Code generation at runtime\n",
    "\n",
    "3. **Schema Information**\n",
    "   - DataFrames have schema (column names and types)\n",
    "   - Enables better optimization decisions\n",
    "   - RDDs are opaque (Spark doesn't know structure)\n",
    "\n",
    "### When to Use What:\n",
    "- **DataFrames**: 95% of use cases (recommended)\n",
    "- **RDDs**: Complex custom logic, low-level control needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: DataFrame vs RDD performance\n",
    "\n",
    "print(\"Comparing DataFrame vs RDD operations:\\n\")\n",
    "\n",
    "# Create test data\n",
    "test_data = spark.range(0, 100000)\n",
    "\n",
    "# DataFrame approach (optimized)\n",
    "print(\"1. DataFrame approach (with Catalyst optimization):\")\n",
    "start = time.time()\n",
    "df_result = test_data \\\n",
    "    .filter(col(\"id\") % 2 == 0) \\\n",
    "    .filter(col(\"id\") < 50000) \\\n",
    "    .count()\n",
    "df_time = time.time() - start\n",
    "print(f\"   Result: {df_result:,} rows\")\n",
    "print(f\"   Time: {df_time:.4f} seconds\")\n",
    "\n",
    "# RDD approach (no optimization)\n",
    "print(\"\\n2. RDD approach (no Catalyst optimization):\")\n",
    "start = time.time()\n",
    "rdd_result = test_data.rdd \\\n",
    "    .filter(lambda row: row[0] % 2 == 0) \\\n",
    "    .filter(lambda row: row[0] < 50000) \\\n",
    "    .count()\n",
    "rdd_time = time.time() - start\n",
    "print(f\"   Result: {rdd_result:,} rows\")\n",
    "print(f\"   Time: {rdd_time:.4f} seconds\")\n",
    "\n",
    "print(f\"\\nüìä Performance:\")\n",
    "print(f\"   DataFrame: {df_time:.4f}s (optimized)\")\n",
    "print(f\"   RDD: {rdd_time:.4f}s (no optimization)\")\n",
    "if rdd_time > df_time:\n",
    "    print(f\"   DataFrame is {rdd_time/df_time:.1f}x faster! ‚úì\")\n",
    "\n",
    "print(\"\\nüí° DataFrames provide automatic optimizations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Partitions\n",
    "\n",
    "### Why Partitions Matter:\n",
    "- Each partition is processed by a **single task**\n",
    "- More partitions = more parallelism\n",
    "- But too many partitions = overhead\n",
    "\n",
    "### Rules of Thumb:\n",
    "- Aim for 2-4 partitions per CPU core\n",
    "- Partition size: 100MB - 1GB\n",
    "- For 8-core cluster: 16-32 partitions\n",
    "\n",
    "### Repartitioning:\n",
    "- **repartition(n)**: Increase or decrease partitions (full shuffle)\n",
    "- **coalesce(n)**: Decrease partitions (minimize shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Working with partitions\n",
    "\n",
    "# Check current partitions\n",
    "print(\"Current partitioning:\")\n",
    "print(f\"  Customers: {customers_df.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"  Transactions: {transactions_df.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Create data with specific number of partitions\n",
    "large_df = spark.range(0, 1000000, numPartitions=8)\n",
    "print(f\"\\nLarge dataset: {large_df.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"Total rows: {large_df.count():,}\")\n",
    "\n",
    "# Repartition to increase parallelism\n",
    "repartitioned = large_df.repartition(16)\n",
    "print(f\"\\nAfter repartition(16): {repartitioned.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Coalesce to reduce partitions\n",
    "coalesced = repartitioned.coalesce(4)\n",
    "print(f\"After coalesce(4): {coalesced.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "print(\"\\nüí° More partitions = more parallel tasks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 7: DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7-9: Additional Content\n",
    "\n",
    "Sections 7 (DataFrame Operations), 8 (Advanced Topics), and 9 (Integration) provide hands-on examples covering:\n",
    "\n",
    "### Section 7: DataFrame Operations\n",
    "- **select()**: Choosing columns from DataFrames\n",
    "- **filter()/where()**: Filtering rows based on conditions\n",
    "- **withColumn()**: Adding/modifying columns\n",
    "- **withColumnRenamed()**: Renaming columns\n",
    "- **orderBy()**: Sorting DataFrames\n",
    "- **groupBy()**: Aggregating data\n",
    "- **union()**: Combining DataFrames with same schema\n",
    "- **join()**: Joining DataFrames on keys\n",
    "- **show()**: Displaying DataFrame contents (action)\n",
    "- **collect()**: Retrieving all rows to driver (action)\n",
    "\n",
    "### Section 8: Advanced Topics\n",
    "- **Spark SQL**: Interactive queries on structured data\n",
    "- **UDFs**: User-Defined Functions for custom logic\n",
    "- **Caching**: Performance optimization strategies\n",
    "\n",
    "### Section 9: Putting It All Together\n",
    "- Complete customer analytics pipeline\n",
    "- Best practices summary\n",
    "- Resources and next steps\n",
    "\n",
    "---\n",
    "\n",
    "**Note for Practice**: Each operation should be practiced with the sample data defined at the beginning of this notebook. Key takeaways:\n",
    "\n",
    "1. **Transformations are lazy** - operations like select, filter, groupBy don't execute until an action is called\n",
    "2. **Actions trigger execution** - show(), collect(), count() cause immediate computation\n",
    "3. **Immutability** - Every transformation creates a new DataFrame\n",
    "4. **filter() and where() are equivalent** - Both filter rows, interchangeable\n",
    "5. **Cache strategically** - Only cache DataFrames used multiple times after expensive operations\n",
    "6. **DataFrames > RDDs** - Better optimization through Catalyst\n",
    "7. **SQL integration** - Can use SQL queries on DataFrames via createOrReplaceTempView()\n",
    "8. **UDF registration** - `spark.udf.register('name', lambda x: x * 2)` for SQL, or use `udf()` decorator for DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary\n",
    "\n",
    "This comprehensive lecture deck has covered all core Spark concepts:\n",
    "\n",
    "### üéØ Core Learning Objectives Achieved:\n",
    "\n",
    "1. **Data-Intensive Applications Foundations**\n",
    "   - 5 V's of Big Data and CRISP-DM methodology\n",
    "   - Team roles and collaboration patterns\n",
    "   - Development best practices and pitfalls\n",
    "   - Decision-making frameworks\n",
    "\n",
    "2. **Apache Spark Architecture**\n",
    "   - Driver and Executor roles\n",
    "   - Job, Stage, Task, and Partition hierarchy\n",
    "   - RDD as the fundamental data structure\n",
    "   - Fault tolerance through lineage\n",
    "\n",
    "3. **Core Spark Concepts**\n",
    "   - Immutability principles\n",
    "   - Lazy evaluation and optimization\n",
    "   - Transformations vs Actions\n",
    "   - DataFrames vs RDDs\n",
    "\n",
    "4. **Practical DataFrame Operations**\n",
    "   - Selection, filtering, and projection\n",
    "   - Column operations and schema modification\n",
    "   - Aggregations and grouping\n",
    "   - Joins and unions\n",
    "   - Actions that trigger execution\n",
    "\n",
    "5. **Advanced Techniques**\n",
    "   - Spark SQL integration\n",
    "   - User-Defined Functions (UDFs)\n",
    "   - Strategic caching for performance\n",
    "   - End-to-end pipeline design\n",
    "\n",
    "### üìö Study Recommendations:\n",
    "\n",
    "- **Review this notebook** section by section\n",
    "- **Run all code examples** in Databricks to see outputs\n",
    "- **Practice with the labs** to reinforce concepts\n",
    "- **Understand the 'why'** behind each concept, not just memorize syntax\n",
    "\n",
    "### üöÄ Ready for Success!\n",
    "\n",
    "You now have a complete reference with executable examples,  \n",
    "conceptual explanations, and best practices for Apache Spark.\n",
    "\n",
    "---\n",
    "\n",
    "**Format**: üìì Jupyter Notebook with markdown and code cells  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
